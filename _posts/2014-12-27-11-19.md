---
layout: post
title: 我爱他身强力壮能劳动 我爱他下地生产真是有本领
date: 2014-12-27 11:19:00
categories: cnblogs
---

<p>用了一天时间重新看了一遍Python的基本语法，晚上就写了两个爬汤站小图片的脚本，什么图片就不细说了。</p>
<ul>
<li>一个用作计划任务，每天定时运行一遍，爬取存档页第一次加载取到的所有图片，保存到以日期命名的文件夹里面，在保存之前检查前一天的文件夹里是不是已经有了，有了就不下了。</li>
<li>另一个是按ID逐页爬取一个博客里的所有图片。</li>
</ul>
<p>现在重看Python，思路跟当初真的完全不一样了，好多当初不懂的现在也全懂了，比如类的问题和命令行操作，这种今非昔比的感觉真爽。</p>
<p>代码因为很简单就不放了吧，毕竟不是当初刚入门的时候了，说说思路。</p>
<hr />
<p>爬虫的基本做法就是读取页面，正则匹配，然后逐个下载。</p>
<p>也就是urllib.urlopen(url).read()，re.findall(regex, str), urllib.urlretrieve(url, file)这么三个函数。</p>
<h3>先说第一个</h3>
<p>存档页显示的都是缩略图，首先翻页面源码，找到这些缩略图的标签写出正则式，像这样：</p>
<blockquote>r'data-imageurl="(http.*\.jpg)"'</blockquote>
<p>这个时候就要感谢汤站的讲究了，每张图片的名称最后都加上了图片的大小，缩略图的格式是&times;&times;&times;_250.jpg，然后打开大图页面发现大图的格式是&times;&times;&times;_1280.jpg，于是果断写个函数把所有链接中的文件名改掉，然后遍历下载就行了。</p>
<p>但是有个问题，存档页用了瀑布流，页面滚到底部的时候才会加载更多图片，用火狐的调试器看了一下，貌似是用js做的，于是不知道该怎么弄了，好在这个博客每天更新得也不是很多，所以也没动力研究其他解决办法。虽然还是有几个想法的。</p>
<h3>然后就开始研究第二个了</h3>
<p>还是多亏汤站的讲究，逐页爬取就更容易了，因为每页的URL是这样的：</p>
<blockquote>
<pre>http://&lt;id&gt;.tumblr.com/page/&lt;number&gt;</pre>
</blockquote>
<p>而且图片也更好找了，还是原生大图：</p>
<blockquote>
<pre>r'href="(http.*1280\.jpg)"'
</pre>
</blockquote>
<p>另外借助别人的点拨还加上了超时重试的功能，首先用socket模块设置全局的超时时间：</p>
<div class="cnblogs_code">
<pre>socket.setdefaulttimeout(30)</pre>
</div>
<p>这样打开页面（urllib.urlopen().read()）时联网超过30秒就会抛出IOError异常，下载图片（urllib.urlretrieve()）超时会抛出socket.timeout异常，只要对异常加以处理就可以了，举个栗子：</p>
<div class="cnblogs_code">
<pre>while True:
    try:
        urllib.urlretrieve(link, file)
    except:
        print '下载超时，五秒后重试&hellip;&hellip;'
        time.sleep(5)
    else:
        break
</pre>
</div>
<p>改成for循环还可以控制重试的次数。</p>
<p>另外要做超时重试是因为爬虫爬取页面过快经常会被网站发现，然后就被ban了，表现出来就是下载被卡住进行不下去，所以超时后休息五秒重新下载。</p>
<p>按理说应该下完每张图都休眠一会。</p>
<p>毕竟人家也怕自己服务器被爬死。</p>
<p>体谅一下。</p>
<!-- This document was created with MarkdownPad, the Markdown editor for Windows (http://markdownpad.com) -->
<p>&nbsp;</p>

<div align=right><a href="https://github.com/mlxy"><font size=1>——本文由博客园搬家工具SRBCnblogs转换而成</font></a></div>